% classifiy_learn
%%% classify learning conditions given parameter samples
%%% with proper K-fold cross-validation in earch loop

%% load tracks from each condition
% load data files
datas = {'/projects/LEIFER/Kevin/Data_learn/N2/data_analysis/Data_app_test2.mat',...
         '/projects/LEIFER/Kevin/Data_learn/N2/data_analysis/Data_nai_test2.mat',...
         '/projects/LEIFER/Kevin/Data_learn/N2/data_analysis/Data_ave_test2.mat'};
     
%% loop through K-folds, conditions, and scaling
rep = 3;  % repetition per fold to make sure that it is the better fit
K = 7;  % K-fold cross-validation
cond = length(datas);  % all exp conditions
n_params = 13;  % number of parameters in our model for now
mle_params = zeros(K, cond, n_params); % K x c x N
null_params = zeros(K, cond, n_params);  % K x c x N, for random walk null model
bin_params = zeros(K, cond, 1);  % K x c x 1, for binomial distribution (chemotaxis index)
Mdls = cell(1,K);  % models for naive bayes for dC clustering

%% %%%%%%%  TRAINING  %%%%%%% 
%%% seperate indices a head of time
ids = cell(1,3);
data_ids = zeros(3, 400);  %place holder for scambled ID for each condition
for ci = 1:cond
    load(datas{ci})  % load all Data tracks
    idvec = randperm(length(Data));
    data_ids(ci,:) = idvec(1:size(data_ids,2));
    Data = Data(data_ids(ci,:));
    data_id = 1:length(Data);  % original track ID
    ids{ci} = crossvalind('Kfold',data_id, K);  % indices for CV
end

%%% training MLEs
for ci = 1:cond  % C conditions
    %%% load a given condition
    load(datas{ci})  % load all Data tracks
    Data = Data(data_ids(ci,:)); % control the number of tracks 
    indices = ids{ci};  % indices for CV (pre-assigned!)
    
    for ki = 1:K  % K-fold
        % using only training set here (flipped the indices for more testing sets)
        test_set = (indices==ki);
        train_set = ~test_set;
        Data_train = Data(train_set); %(test_set);

        % training, with small repetition
        x_temp = zeros(rep,n_params);
        fv_temp = zeros(1,rep);
        for ri = 1:rep   % test with repreats to find the max MLE
            [x_MLE, fval] = MLE_mGLM(Data_train);
            fv_temp(ri) = fval;
            x_temp(ri,:) = x_MLE;
        end
        
        % record MLE
        [~,pos] = min(fv_temp);
        x_MLE = x_temp(pos,:);
        mle_params(ki, ci, :) = x_MLE;  % can replace this with repetition and choose best fit in the future..........
        [null_mle, null_fval] = MLE_randwalk(Data_train);  % random walk MLE
        null_params(ki,ci, :) = null_mle;
        [p_mle, p_fval] = MLE_binomial(Data_train);  % binomial chemotaxis MEL
        bin_params(ki,ci,1) = p_mle;
    
    end
end

% Naive Bayes for dC calculation
for ki = 1:K
    dc_train = [];
    datal_c = zeros(1,3);
    for ci = 1:cond  % C conditions
        load(datas{ci})  % load all Data tracks
        Data = Data(data_ids(ci,:)); % control the number of tracks 
        indices = ids{ci}; 
        test_set = (indices==ki);
        train_set = ~test_set;
        Data_train = Data(train_set);
        datal_c(ci) = sum(train_set);
        for dd = 1:datal_c(ci)
            dc_train = [dc_train   (Data_train(dd).dc(end) - Data_train(dd).dc(1))];
        end
    end
    labels = [make_label(datal_c(1),'app')', make_label(datal_c(2),'nai')', make_label(datal_c(3),'ave')']';
    mss = dc_train';
    Mdls{ki} = fitcnb(mss,labels,'ClassNames',{'app','nai','ave'});
end

%% %%%%%%%  TESTING  %%%%%%% 
%%% testing with MLEs
scal = 9;  % data length portions
min_scal = 0.4;  % 0-1, for rescaling test data
rep_samp = 10;  % repeat sub-sampling for better average performance
cv_class = zeros(K, cond, scal, rep_samp); % K x c x T x s
data_len = zeros(K, cond, scal, rep_samp); % K x c x T x s
bin_class = zeros(K, cond, scal, rep_samp); % K x c x T x s
cv_perf = zeros(cond, scal, rep_samp);  % c x T x s, this is averaged over K folds
cv_perf_bin = zeros(cond, scal, rep_samp);  % c x T x s,  for binomial comparison
testLL = zeros(K,cond, 3);  % record the test LL (three conditions for analysis)

for ci = 1:cond  % C conditions
    %%% load a given condition
    load(datas{ci})  % load all Data tracks
    Data(data_ids(ci,:));
    indices = ids{ci};  % indices for CV (pre-assigned!)
    
    for ki = 1:K  % K-fold
        % using only training set here
        test_set = (indices==ki);
        train_set = ~test_set;
        Data_test = Data(test_set);%(train_set);
    
        % record the test LL
        x_MLE = squeeze(mle_params(ki, ci, :))';  % recall MLE
        [xx_test, yy_test, mask_test] = data2xy(Data_test);
        
        %%% testing null models
%         x_null = [x_MLE(1:2), zeros(1,4),x_MLE(7),0, 1, 0, 1, x_MLE(12),x_MLE(13)];
%         x_null = [x_MLE(1:2),zeros(1,4), x_MLE(7:13)];
%         x_null = [x_MLE(1:7),zeros(1,4), x_MLE(12:13)];
%         x_null = [x_MLE(1),mean([x_MLE(2),x_MLE(7)]),zeros(1,4),mean([x_MLE(2),x_MLE(7)]),zeros(1,4), x_MLE(12:13)];
%         pturn = length(find(abs(yy_test)>100))/length(yy_test);
%         x_null = [x_MLE(1), pturn, zeros(1,4), pturn, zeros(1,4), x_MLE(12:13)];
        info_norm_factor = (sum(mask_test)*(5/14)) / log(2);
        x_null = squeeze(null_params(ki, ci, :))';  % individually fitted null model parameters
        x_wo_K = [x_MLE(1:2), zeros(1,4), x_MLE(7), zeros(1,4), x_MLE(12:13)];
        testLL(ki,ci,1) = -(pop_nLL(x_MLE, Data_test) - pop_nLL_randwalk(x_null, Data_test) ) / info_norm_factor;  % full model - randomwalk
        x_wo_Kc = [x_MLE(1:2), zeros(1,4), x_MLE(7:13)];
        testLL(ki,ci,2) = -(pop_nLL(x_wo_Kc, Data_test) - pop_nLL_randwalk(x_null, Data_test) ) / info_norm_factor;   % model w/o Kc - random
        x_wo_Kcp = [x_MLE(1:7),zeros(1,2), x_MLE(10:13)];
        testLL(ki,ci,3) = -(pop_nLL(x_wo_Kcp, Data_test) - pop_nLL_randwalk(x_null, Data_test) ) / info_norm_factor;  % model w/o Kcp - random
        
        % testing scaled with time and bootstrap
        for ri = 1:rep_samp
            scal_vec = floor(linspace(1, min_scal*length(Data_test), scal));
            %fliplr(floor(min_scal./[1:scal].*length(Data_test)));  % scaled data length, can do proper tiled timing sampling in the future..........
            data_select_vec = [1:length(Data_test)];
            for si = 1:scal
                samps = randsample(data_select_vec, scal_vec(si));  % sample without replacement
                cv_class(ki, ci, si, ri) = argmaxLL(Data_test(samps), squeeze(mle_params(ki,:,:)));  % selection
                bin_class(ki, ci, si, ri) = argmaxLL_bin(Data_test(samps), squeeze(bin_params(ki,:,:)));  % compared to binomial model
                [xx, yy, mm] = data2xy(Data_test(samps));  % concatenate data
                data_len(ki, ci, si, ri) = data_len(ki, ci, si, ri) + length(yy);  % average length(?)..........
            end
        end
    end
end

%%% lastly, quantify performance
for ci = 1:cond
    for si = 1:scal
        for ri = 1:rep_samp
            cv_perf(ci,si,ri) = length(find(cv_class(:, ci, si,ri)==ci))/K;
            cv_perf_bin(ci,si,ri) = length(find(bin_class(:, ci, si,ri)==ci))/K;
        end
    end
end

cv_nbc = zeros(K, scal, rep_samp);  % K x T x s
%%% Naiv Bayes testing
for ri = 1:rep_samp
for ki = 1:K
    dc_test = [];
    datal_c = zeros(1,3);
    for ci = 1:cond  % C conditions
        load(datas{ci})  % load all Data tracks
        Data = Data(data_ids(ci,:)); % control the number of tracks 
        test_set = (indices==ki);
        train_set = ~test_set;
        Data_test = Data(test_set);
        datal_c(ci) = sum(test_set);
        for dd = 1:datal_c(ci)
            dc_test = [dc_test   (Data_test(dd).dc(end) - Data_test(dd).dc(1))];
        end
    end
    labels = [make_label(datal_c(1),'app')', make_label(datal_c(2),'nai')', make_label(datal_c(3),'ave')']';
    scal_vec = floor(linspace(1, min_scal*length(Data_test), scal));
    data_select_vec = [1:length(dc_test)];
    for si = 1:scal
        samps = randsample(data_select_vec, scal_vec(si));  % sample without replacement
        predicted_labels = predict(Mdls{ki}, dc_test(samps)');
%         cv_nbc(ki, si, ri) = sum(predicted_labels == labels(samps)) / numel(scal_vec(si));
        cv_nbc(ki, si, ri) = sum(cellfun(@isequal, predicted_labels, labels(samps))) / (scal_vec(si));
    end
end
end
cv_nbc = squeeze(mean(cv_nbc,1)); 

%%
figure;
% tlt = data_len; %squeeze(mean(data_len,2));
% plot(tlt(:)*5/14/60/60,cv_class(:),'o')
% xlabel('data length (hour)')
% ylabel('class')

rtime = squeeze(mean(mean(mean(data_len,1),2),4))*5/14/60/60;
figure;
plot(rtime, mean(cv_perf,3),'-o')
hold on
plot(rtime, mean(mean(cv_perf,3)),'k-o')
errorbar(rtime, mean(mean(cv_perf,3)), std(mean(cv_perf,3))/sqrt(K), '.k')

plot(rtime, mean(mean(cv_perf_bin,3)),'g--')
errorbar(rtime, mean(mean(cv_perf_bin,3)), std(mean(cv_perf_bin,3))./sqrt(K.*scal_vec), '.g')

plot(rtime, mean((cv_nbc),2),'r--')
errorbar(rtime, mean((cv_nbc),2), std((cv_nbc),0,2)'./sqrt(K.*scal_vec), '.r')

xlabel('mean data length (hr)')
ylabel('cross-validation (ratio)')
legend({'appetitive','naive','aversive','mean prediction', 'binomial prediction', '\Delta C'})
set(gcf,'color','w'); set(gca,'Fontsize',20);


%% some post analysis for variability!
ttl = {'appetitive','naive','aversive'};
tt = [1:length(cosBasis)]*5/14;
figure
[cosBasis, tgrid, basisPeaks] = makeRaisedCosBasis(4, [0, 8], 1.3);
for cc = 1:3
    subplot(1,3,cc);
for ii = 1:K
    temp = squeeze(mle_params(ii,cc,3:6))'*cosBasis';
    plot(tt,temp)
    hold on
end; title(ttl{cc})
end

figure; 
xx = 0:length(cosBasis)-1;
for cc = 1:3
    subplot(1,3,cc)
for ii = 1:K
    amp = mle_params(ii,cc,8); tau = mle_params(ii,cc,9); temp = (-amp*exp(-xx/tau));
    plot(tt,temp)
    hold on
end; title(ttl{cc})
end

%% information analysis
figure
col = {'b','k','r'};
tils = {'full','K_c','K_{c^\perp}'};
for nn = 1:3
    subplot(1,3,nn)
    temp_m = squeeze(mean(testLL(:,:,nn),1));
    temp_s = squeeze(std(testLL(:,:,nn),0,1))/sqrt(7);
    for cc = 1:3
        bar(cc, temp_m(cc), 'FaceColor',col{cc})
        hold on
        errorbar(cc, temp_m(cc), temp_s(cc), 'k.');
    end
    title(tils{nn})
    xticklabels([]);
end

%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% classify with log-likelihood!
% load mle parameters
% mle_params = zeros(3,13);
% test = load('/projects/LEIFER/Kevin/Data_learn/app_param_new.mat');
% mle_params(1,:) = test.x;
% test = load('/projects/LEIFER/Kevin/Data_learn/nai_param_new.mat');
% mle_params(2,:) = test.x;
% test = load('/projects/LEIFER/Kevin/Data_learn/ave_param_new.mat');
% mle_params(3,:) = test.x;
%      
% % CV settings
% rep = 20;  % repeat cross-validations
% scal = 5;  % data length portions
% cv_class = zeros(rep,scal);  % repeats x data length
% datals = cv_class*1;  % record actual data length
% cv_perf = zeros(3,scal);  % record performance (%) across repeats
% 
% for dd = 1:3
%     load(datas{dd})
%     Datai = Data(1:end);  % load Data structure
%     scal_vec = fliplr(floor(.5./[1:scal].*length(Datai)));  % scaled data length
%     data_select_vec = [1:length(Datai)];  % select from track ID
% for rr = 1:rep
%     for sc = 1:scal
%         samps = randsample(data_select_vec, scal_vec(sc));  % sample without replacement
%         cv_class(rr, sc) = argmaxLL(Datai(samps), mle_params, cosBasis);  % selection
%         [xx, yy, mm] = data2xy(Datai(samps));  % concatenate data
%         datals(rr, sc) = length(yy);
%     end    
% end
% 
% for sc = 1:scal
%     cv_perf(dd,sc) = length(find(cv_class(:,sc)==dd))/rep;
% end
%     
% end
% 
% cv_class

%%
function [x_MLE, fval] = MLE_mGLM(Data_train)
    [xx_train, yy_train, mask_train] = data2xy(Data_train);
    nB = 4;
    [cosBasis, tgrid, basisPeaks] = makeRaisedCosBasis(nB, [0, 8], 1.3);
    ang_fit = yy_train;
    dcp_fit = xx_train(2,:);
    ddc_fit = xx_train(1,:);
    trials_fit = mask_train;
    lfun = @(x)nLL_kernel_hist2(x, ang_fit, dcp_fit, ddc_fit, cosBasis, .1, trials_fit);  %nLL_kernel_hist2
    opts = optimset('display','iter');
    LB = [1e-0, 1e-5, ones(1,nB)*-inf, 0.0 -inf, 1e-0, -inf, 1e-1, 1e-0*10, 0.1];
    UB = [200, 1., ones(1,nB)*inf, 0.1, inf, 50, inf, 100, 50, 1];
    prs0 = [50, 0.1, randn(1,nB)*10, 0.01, -10, 25, 10, 25, 5, 1.];
%     LB = [1e-0, 1e-5, ones(1,nB)*-inf, 0.0 -inf, 1e-0, -inf, 1e-1, 1e-0*10, -inf];
%     UB = [200, 1., ones(1,nB)*inf, 0.1, inf, 50, inf, 100, 50, inf];
%     prs0 = [50, 0.1, randn(1,nB)*10, 0.01, -10, 25, 10, 25, 5, 0];
    prs0 = prs0 + prs0.*randn(1,length(UB))*0.1;
    [x,fval,EXITFLAG,OUTPUT,LAMBDA,GRAD,HESSIAN] = fmincon(lfun,prs0,[],[],[],[],LB,UB,[],opts);
    x_MLE = x;
end

function [x_MLE, fval] = MLE_randwalk(Data_train)
    [xx_train, yy_train, mask_train] = data2xy(Data_train);
    nB = 4;
    [cosBasis, tgrid, basisPeaks] = makeRaisedCosBasis(nB, [0, 8], 1.3);
    ang_fit = yy_train;
    dcp_fit = xx_train(2,:);
    ddc_fit = xx_train(1,:);
    trials_fit = mask_train;
    lfun = @(x)nLL_randomwalk(x, ang_fit, dcp_fit, ddc_fit, cosBasis, .1, trials_fit);
    opts = optimset('display','iter');
    LB = [1e-0, 0, ones(1,nB)*-inf, 0.0 -inf, 1e-0, -inf, 1e-1, 1e-0*10, 0.1];
    UB = [200, 1., ones(1,nB)*inf, 0.1, inf, 50, inf, 100, 50, 1];
    prs0 = [50, 0.05, randn(1,nB)*10, 0.01, -10, 25, 10, 25, 5, 1.];
    prs0 = prs0 + prs0.*randn(1,length(UB))*0.1;
    [x,fval,EXITFLAG,OUTPUT,LAMBDA,GRAD,HESSIAN] = fmincon(lfun,prs0,[],[],[],[],LB,UB,[],opts);
    x_MLE = x;
end

function [x_MLE, fval] = MLE_binomial(Data_train)
    lfun = @(x)pop_nLL_binomial(x, Data_train);
    opts = optimset('display','iter');
    LB = 0;
    UB = 1;
    prs0 = 0.5;
    [x,fval,EXITFLAG,OUTPUT,LAMBDA,GRAD,HESSIAN] = fmincon(lfun,prs0,[],[],[],[],LB,UB,[],opts);
    x_MLE = x;
end

function predict_lambda = argmaxLL(data, mle_params)
    [Basis, tgrid, basisPeaks] = makeRaisedCosBasis(4, [0, 8], 1.3);
    [xx, ang_fit, trials_fit] = data2xy(data);  % turn data sturcute into vectors for plug-in evaluation
    dcp_fit = xx(2,:);  % dcp concatented
    ddc_fit = xx(1,:);  % dc concatentated
    lls = zeros(1,3);  % three conditions
    for c = 1:3
        lls(c) = -nLL_kernel_hist2(mle_params(c,:), ang_fit, dcp_fit, ddc_fit, Basis, .1, trials_fit) / length(xx);  % normalize by length?
        
%         mle_c = mle_params(c,:);
%         x_null = [mle_c(1:2), zeros(1,4),mle_c(7),0, 1, 0, 1, mle_c(12),mle_c(13)];
%         lls(c) = - ( nLL_kernel_hist2(mle_c, ang_fit, dcp_fit, ddc_fit, Basis, .1, trials_fit) - ...
%                      nLL_kernel_hist2(x_null, ang_fit, dcp_fit, ddc_fit, Basis, .1, trials_fit) ) / length(xx);   % gain of info compared to random walk
    end
%     lls
    predict_lambda = argmax(lls);
end

function predict_lambda = argmaxLL_bin(data, bin_params)
    lls = zeros(1,3);  % three conditions
    for c = 1:3
        lls(c) = -pop_nLL_binomial(bin_params(c), data);
    end
    predict_lambda = argmax(lls);
end

function labels= make_label(ns, label)
    labels = cell(ns,1);
    for ii = 1:ns
        labels{ii} = label;
    end
end